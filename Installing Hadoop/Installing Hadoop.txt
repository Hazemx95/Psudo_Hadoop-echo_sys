# download Ubuntu server
https://ubuntu.com/download

# description for ubuntu server 
lsb_release -a


# Download Java JDK 1.8 from the below
https://javadl.oracle.com/webapps/download/AutoDL?BundleId=245469_4d5417147a92418ea8b615e228bb6935

0. Prereq
sudo apt update
sudo apt upgrade 
OR 
sudo apt update && sudo apt full-upgrade -y
# first command when any linux comman must be run first in this paths
echo $PATH
PATH=.:$PATH
# for creation config file 
touch config 
dpkg - l| openssh 
systemctl status ssh
cd /etc/ssh
man ssh-keygen

#nano authorized_keys (bulk here public key )
# to clear authorized_keys 
echo "" >authorized_keys

# have amzing utility to treat with files and using all the configuration in ssh 
#scp --> file transfer
man scp
scp /home/hazem/file HadoopLinux:/home/hazem
scp -r
 
1. Create a hadoop user and configure passwordless ssh
# to check database of groups 
sudo cat /etc/group
groups
sudo addgroup hadoop
sudo adduser --ingroup hadoop hduser   
OR
cat /etc/passwd
sudo useradd -m -c " " 
sudo userdel -r (remove home directory)
sudo usermod -aG hadoop hduser
groups hduser
sudo usermod -rG hadoop hduser
sudo groupdel hadoop
cat /etc/passwd | head -n 10 | wc -l
us01=$(cat /etc/passwd | head -n 10 | wc -l)
sudo visudo
hduser ALL=(ALL:ALL) ALL
 
su - hduser 
 
2. Install Java 8 Ubuntu 18.04
java -version
 
cd /usr/local 
sudo wget 'https://files-cdn.liferay.com/mirrors/download.oracle.com/otn-pub/java/jdk/8u221-b11/jdk-8u221-linux-x64.tar.gz'
wget -o (rename after download)
wget -i (put all your links in this file and recuarsf downaload each file )
or curl (client URl treat with rest API'S (query with web application) )
man curl
or 
w3m (with graphically tear with terminal )
w3m google.com
 
sudo tar -xzvf jdk-8u221-linux-x64.tar.gz
sudo mv jdk1.8.0_221/ java 
 
ls 
cd ~
sudo vim ~/.bashrc 
 
export JAVA_HOME=/usr/local/java
export PATH=:$PATH:/usr/local/java/bin
 
sudo update-alternatives --install "/usr/bin/java" "java" "/usr/local/java/bin/java" 1
sudo update-alternatives --install "/usr/bin/javac" "javac" "/usr/local/java/bin/javac" 1
sudo update-alternatives --install "/usr/bin/javaws" "javaws" "/usr/local/java/bin/javaws" 1
 
sudo update-alternatives --set java /usr/local/java/bin/java
sudo update-alternatives --set javac /usr/local/java/bin/javac
sudo update-alternatives --set javaws /usr/local/java/bin/javaws
 
java -version
 
3. Install SSH 
sudo apt-get update
sudo apt-get install openssh-server openssh-client
sudo ufw allow 22

# the below command may fail in docker containers. try 'service ssh restart'
sudo apt show openssh-client
sudo systemctl status ssh 
sudo systemctl restart ssh

sudo apt-get install ssh
sudo apt-get install rsync
 
 
ssh-keygen -t rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod og-wx ~/.ssh/authorized_keys
sudo apt-get update

ssh HadoopLinux
OR 

ssh localhost
 
#to edit in openssh server (mean configuration) 
cat /etc/ssh/sshd (d refer to server )
3. Download and Install Hadoop
 
cd /usr/local 
sudo wget https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz

sudo tar -xzvf hadoop-3.3.6.tar.gz
sudo mv hadoop-3.3.6 hadoop
 
sudo chown -R hduser:hadoop /usr/local/hadoop
sudo chmod -R 777 /usr/local/hadoop
 
sudo vim /etc/sysctl.conf
 
net.ipv6.conf.all.disable_ipv6=1
net.ipv6.conf.default.disable_ipv6=1
net.ipv6.conf.lo.disable_ipv6=1
 
sudo vim ~/.bashrc 
 
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export HADOOP_MAPRED_HOME=/usr/local/hadoop
export HADOOP_COMMON_HOME=/usr/local/hadoop
export HADOOP_HDFS_HOME=/usr/local/hadoop
export YARN_HOME=/usr/local/hadoop
export PATH=$PATH:/usr/local/hadoop/bin
export PATH=$PATH:/usr/local/hadoop/sbin
 
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"


 
4. Setup hadoop configuration 
cd /usr/local/hadoop/etc/hadoop/
 
sudo vim hadoop-env.sh
 
export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true
export JAVA_HOME=/usr/local/java
export HADOOP_HOME_WARN_SUPPRESS="TRUE"
export HADOOP_ROOT_LOGGER="WARN,DRFA"

# also add the following when installing in docker container for root user
export HDFS_NAMENODE_USER="root"
export HDFS_DATANODE_USER="root"
export HDFS_SECONDARYNAMENODE_USER="root"
export YARN_RESOURCEMANAGER_USER="root"
export YARN_NODEMANAGER_USER="root"
 
sudo vim yarn-site.xml
 
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
 
sudo vim hdfs-site.xml
 
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
<property>
<name>dfs.namenode.name.dir</name>
<value>file:/usr/local/hadoop/yarn_data/hdfs/namenode</value>
</property>
<property>
<name>dfs.datanode.data.dir</name>
<value>file:/usr/local/hadoop/yarn_data/hdfs/datanode</value>
</property>
 
sudo vim core-site.xml
 
<property>
<name>hadoop.tmp.dir</name>
<value>/app/hadoop/tmp</value>
</property>
<property>
<name>fs.default.name</name>
<value>hdfs://localhost:9000</value>
</property>
 
 
sudo vim mapred-site.xml
 
<property>
<name>mapred.framework.name</name>
<value>yarn</value>
</property>
<property>
<name>mapreduce.jobhistory.address</name>
<value>localhost:10020</value>
</property>
 
sudo mkdir -p /app/hadoop/tmp
sudo chown -R hduser:hadoop /app/hadoop/tmp
sudo chmod -R 777 /app/hadoop/tmp
 
sudo mkdir -p /usr/local/hadoop/yarn_data/hdfs/namenode

sudo mkdir -p /usr/local/hadoop/yarn_data/hdfs/datanode

sudo chmod -R 777 /usr/local/hadoop/yarn_data/hdfs/namenode

sudo chmod -R 777 /usr/local/hadoop/yarn_data/hdfs/datanode

sudo chown -R hduser:hadoop /usr/local/hadoop/yarn_data/hdfs/namenode

sudo chown -R hduser:hadoop /usr/local/hadoop/yarn_data/hdfs/datanode

 
source ~/.bashrc
 
hdfs namenode -format
echo $?
 
start-dfs.sh 
start-yarn.sh
 
jps
 
namenode 9870
http://localhost:9870/
 
cluster info (yarn)
http://localhost:8042/
 
hadoop nodes 9864 (data nondes)
http://localhost:9864/
 
 
bin/hdfs dfs -mkdir /user
bin/hdfs dfs -mkdir /user/hadoop

