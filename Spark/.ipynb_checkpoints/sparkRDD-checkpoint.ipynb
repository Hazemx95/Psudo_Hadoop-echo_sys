{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6c6dbac620>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[spark documentaion for RDD](https://spark.apache.org/docs/3.5.2/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(1,51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now i need to upload this data in RDD\n",
    "\n",
    "rdd0 = sc.parallelize(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "print (type (rdd0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = rdd0.map(lambda x:x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd1.map(lambda x :x**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6c6dbac620>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 64,\n",
       " 729,\n",
       " 4096,\n",
       " 15625,\n",
       " 46656,\n",
       " 117649,\n",
       " 262144,\n",
       " 531441,\n",
       " 1000000,\n",
       " 1771561,\n",
       " 2985984,\n",
       " 4826809,\n",
       " 7529536,\n",
       " 11390625,\n",
       " 16777216,\n",
       " 24137569,\n",
       " 34012224,\n",
       " 47045881,\n",
       " 64000000,\n",
       " 85766121,\n",
       " 113379904,\n",
       " 148035889,\n",
       " 191102976,\n",
       " 244140625,\n",
       " 308915776,\n",
       " 387420489,\n",
       " 481890304,\n",
       " 594823321,\n",
       " 729000000,\n",
       " 887503681,\n",
       " 1073741824,\n",
       " 1291467969,\n",
       " 1544804416,\n",
       " 1838265625,\n",
       " 2176782336,\n",
       " 2565726409,\n",
       " 3010936384,\n",
       " 3518743761,\n",
       " 4096000000,\n",
       " 4750104241,\n",
       " 5489031744,\n",
       " 6321363049,\n",
       " 7256313856,\n",
       " 8303765625,\n",
       " 9474296896,\n",
       " 10779215329,\n",
       " 12230590464,\n",
       " 13841287201,\n",
       " 15625000000]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4,\n",
       " 9,\n",
       " 16,\n",
       " 25,\n",
       " 36,\n",
       " 49,\n",
       " 64,\n",
       " 81,\n",
       " 100,\n",
       " 121,\n",
       " 144,\n",
       " 169,\n",
       " 196,\n",
       " 225,\n",
       " 256,\n",
       " 289,\n",
       " 324,\n",
       " 361,\n",
       " 400,\n",
       " 441,\n",
       " 484,\n",
       " 529,\n",
       " 576,\n",
       " 625,\n",
       " 676,\n",
       " 729,\n",
       " 784,\n",
       " 841,\n",
       " 900,\n",
       " 961,\n",
       " 1024,\n",
       " 1089,\n",
       " 1156,\n",
       " 1225,\n",
       " 1296,\n",
       " 1369,\n",
       " 1444,\n",
       " 1521,\n",
       " 1600,\n",
       " 1681,\n",
       " 1764,\n",
       " 1849,\n",
       " 1936,\n",
       " 2025,\n",
       " 2116,\n",
       " 2209,\n",
       " 2304,\n",
       " 2401,\n",
       " 2500]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(4)\n",
    "# rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = rdd1.map(lambda x:x / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = rdd1.map(lambda x : x % 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "cell": {
        "!": "OSMagics",
        "HTML": "Other",
        "SVG": "Other",
        "bash": "Other",
        "capture": "ExecutionMagics",
        "code_wrap": "ExecutionMagics",
        "debug": "ExecutionMagics",
        "file": "Other",
        "html": "DisplayMagics",
        "javascript": "DisplayMagics",
        "js": "DisplayMagics",
        "latex": "DisplayMagics",
        "markdown": "DisplayMagics",
        "perl": "Other",
        "prun": "ExecutionMagics",
        "pypy": "Other",
        "python": "Other",
        "python2": "Other",
        "python3": "Other",
        "ruby": "Other",
        "script": "ScriptMagics",
        "sh": "Other",
        "svg": "DisplayMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "writefile": "OSMagics"
       },
       "line": {
        "alias": "OSMagics",
        "alias_magic": "BasicMagics",
        "autoawait": "AsyncMagics",
        "autocall": "AutoMagics",
        "automagic": "AutoMagics",
        "autosave": "KernelMagics",
        "bookmark": "OSMagics",
        "cat": "Other",
        "cd": "OSMagics",
        "clear": "KernelMagics",
        "code_wrap": "ExecutionMagics",
        "colors": "BasicMagics",
        "conda": "PackagingMagics",
        "config": "ConfigMagics",
        "connect_info": "KernelMagics",
        "cp": "Other",
        "debug": "ExecutionMagics",
        "dhist": "OSMagics",
        "dirs": "OSMagics",
        "doctest_mode": "BasicMagics",
        "ed": "Other",
        "edit": "KernelMagics",
        "env": "OSMagics",
        "gui": "BasicMagics",
        "hist": "Other",
        "history": "HistoryMagics",
        "killbgscripts": "ScriptMagics",
        "ldir": "Other",
        "less": "KernelMagics",
        "lf": "Other",
        "lk": "Other",
        "ll": "Other",
        "load": "CodeMagics",
        "load_ext": "ExtensionMagics",
        "loadpy": "CodeMagics",
        "logoff": "LoggingMagics",
        "logon": "LoggingMagics",
        "logstart": "LoggingMagics",
        "logstate": "LoggingMagics",
        "logstop": "LoggingMagics",
        "ls": "Other",
        "lsmagic": "BasicMagics",
        "lx": "Other",
        "macro": "ExecutionMagics",
        "magic": "BasicMagics",
        "mamba": "PackagingMagics",
        "man": "KernelMagics",
        "matplotlib": "PylabMagics",
        "micromamba": "PackagingMagics",
        "mkdir": "Other",
        "more": "KernelMagics",
        "mv": "Other",
        "notebook": "BasicMagics",
        "page": "BasicMagics",
        "pastebin": "CodeMagics",
        "pdb": "ExecutionMagics",
        "pdef": "NamespaceMagics",
        "pdoc": "NamespaceMagics",
        "pfile": "NamespaceMagics",
        "pinfo": "NamespaceMagics",
        "pinfo2": "NamespaceMagics",
        "pip": "PackagingMagics",
        "popd": "OSMagics",
        "pprint": "BasicMagics",
        "precision": "BasicMagics",
        "prun": "ExecutionMagics",
        "psearch": "NamespaceMagics",
        "psource": "NamespaceMagics",
        "pushd": "OSMagics",
        "pwd": "OSMagics",
        "pycat": "OSMagics",
        "pylab": "PylabMagics",
        "qtconsole": "KernelMagics",
        "quickref": "BasicMagics",
        "recall": "HistoryMagics",
        "rehashx": "OSMagics",
        "reload_ext": "ExtensionMagics",
        "rep": "Other",
        "rerun": "HistoryMagics",
        "reset": "NamespaceMagics",
        "reset_selective": "NamespaceMagics",
        "rm": "Other",
        "rmdir": "Other",
        "run": "ExecutionMagics",
        "save": "CodeMagics",
        "sc": "OSMagics",
        "set_env": "OSMagics",
        "store": "StoreMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "tb": "ExecutionMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "unalias": "OSMagics",
        "unload_ext": "ExtensionMagics",
        "who": "NamespaceMagics",
        "who_ls": "NamespaceMagics",
        "whos": "NamespaceMagics",
        "xdel": "NamespaceMagics",
        "xmode": "BasicMagics"
       }
      },
      "text/plain": [
       "Available line magics:\n",
       "%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %code_wrap  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %mamba  %man  %matplotlib  %micromamba  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n",
       "\n",
       "Available cell magics:\n",
       "%%!  %%HTML  %%SVG  %%bash  %%capture  %%code_wrap  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n",
       "\n",
       "Automagic is ON, % prefix IS NOT needed for line magics."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 ms, sys: 8.9 ms, total: 10.1 ms\n",
      "Wall time: 350 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.11 ms, sys: 1.06 ms, total: 7.17 ms\n",
      "Wall time: 216 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25,\n",
       " 1.0,\n",
       " 2.25,\n",
       " 4.0,\n",
       " 6.25,\n",
       " 9.0,\n",
       " 12.25,\n",
       " 16.0,\n",
       " 20.25,\n",
       " 25.0,\n",
       " 30.25,\n",
       " 36.0,\n",
       " 42.25,\n",
       " 49.0,\n",
       " 56.25,\n",
       " 64.0,\n",
       " 72.25,\n",
       " 81.0,\n",
       " 90.25,\n",
       " 100.0,\n",
       " 110.25,\n",
       " 121.0,\n",
       " 132.25,\n",
       " 144.0,\n",
       " 156.25,\n",
       " 169.0,\n",
       " 182.25,\n",
       " 196.0,\n",
       " 210.25,\n",
       " 225.0,\n",
       " 240.25,\n",
       " 256.0,\n",
       " 272.25,\n",
       " 289.0,\n",
       " 306.25,\n",
       " 324.0,\n",
       " 342.25,\n",
       " 361.0,\n",
       " 380.25,\n",
       " 400.0,\n",
       " 420.25,\n",
       " 441.0,\n",
       " 462.25,\n",
       " 484.0,\n",
       " 506.25,\n",
       " 529.0,\n",
       " 552.25,\n",
       " 576.0,\n",
       " 600.25,\n",
       " 625.0]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RDD in module pyspark.rdd object:\n",
      "\n",
      "class RDD(typing.Generic)\n",
      " |  RDD(jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n",
      " |\n",
      " |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
      " |  Represents an immutable, partitioned collection of elements that can be\n",
      " |  operated on in parallel.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      RDD\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __add__(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n",
      " |      Return the union of this RDD and another one.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> (rdd + rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |\n",
      " |  __getnewargs__(self) -> NoReturn\n",
      " |\n",
      " |  __init__(self, jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  aggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U]) -> ~U\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given combine functions and a neutral \"zero\n",
      " |      value.\"\n",
      " |\n",
      " |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify ``t2``.\n",
      " |\n",
      " |      The first function (seqOp) can return a different result type, U, than\n",
      " |      the type of this RDD. Thus, we need one operation for merging a T into\n",
      " |      an U and one operation for merging two U\n",
      " |\n",
      " |      .. versionadded:: 1.1.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      zeroValue : U\n",
      " |          the initial value for the accumulated result of each partition\n",
      " |      seqOp : function\n",
      " |          a function used to accumulate results within a partition\n",
      " |      combOp : function\n",
      " |          an associative function used to combine results from different partitions\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      U\n",
      " |          the aggregated result\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduce`\n",
      " |      :meth:`RDD.fold`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      " |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (10, 4)\n",
      " |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (0, 0)\n",
      " |\n",
      " |  aggregateByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~U, seqFunc: Callable[[~U, ~V], ~U], combFunc: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f6ca241f9c0>) -> 'RDD[Tuple[K, U]]'\n",
      " |      Aggregate the values of each key, using given combine functions and a neutral\n",
      " |      \"zero value\". This function can return a different result type, U, than the type\n",
      " |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
      " |      a U and one operation for merging two U's, The former operation is used for merging\n",
      " |      values within a partition, and the latter is used for merging values between\n",
      " |      partitions. To avoid memory allocation, both of these functions are\n",
      " |      allowed to modify and return their first argument instead of creating a new U.\n",
      " |\n",
      " |      .. versionadded:: 1.1.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      zeroValue : U\n",
      " |          the initial value for the accumulated result of each partition\n",
      " |      seqFunc : function\n",
      " |          a function to merge a V into a U\n",
      " |      combFunc : function\n",
      " |          a function to combine two U's into a single one\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          function to compute the partition index\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and the aggregated result for each key\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduceByKey`\n",
      " |      :meth:`RDD.combineByKey`\n",
      " |      :meth:`RDD.foldByKey`\n",
      " |      :meth:`RDD.groupByKey`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
      " |      >>> seqFunc = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      " |      >>> combFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      " |      >>> sorted(rdd.aggregateByKey((0, 0), seqFunc, combFunc).collect())\n",
      " |      [('a', (3, 2)), ('b', (1, 1))]\n",
      " |\n",
      " |  barrier(self: 'RDD[T]') -> 'RDDBarrier[T]'\n",
      " |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n",
      " |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n",
      " |      entire stage and relaunch all tasks for this stage.\n",
      " |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n",
      " |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n",
      " |\n",
      " |      .. versionadded:: 2.4.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDDBarrier`\n",
      " |          instance that provides actions within a barrier stage.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :class:`pyspark.BarrierTaskContext`\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      For additional information see\n",
      " |\n",
      " |      - `SPIP: Barrier Execution Mode <http://jira.apache.org/jira/browse/SPARK-24374>`_\n",
      " |      - `Design Doc <https://jira.apache.org/jira/browse/SPARK-24582>`_\n",
      " |\n",
      " |      This API is experimental\n",
      " |\n",
      " |  cache(self: 'RDD[T]') -> 'RDD[T]'\n",
      " |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          The same :class:`RDD` with storage level set to `MEMORY_ONLY`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.persist`\n",
      " |      :meth:`RDD.unpersist`\n",
      " |      :meth:`RDD.getStorageLevel`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd2 = rdd.cache()\n",
      " |      >>> rdd2 is rdd\n",
      " |      True\n",
      " |      >>> str(rdd.getStorageLevel())\n",
      " |      'Memory Serialized 1x Replicated'\n",
      " |      >>> _ = rdd.unpersist()\n",
      " |\n",
      " |  cartesian(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n",
      " |      Return the Cartesian product of this RDD and another one, that is, the\n",
      " |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n",
      " |      ``b`` is in `other`.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          the Cartesian product of this :class:`RDD` and another one\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`pyspark.sql.DataFrame.crossJoin`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2])\n",
      " |      >>> sorted(rdd.cartesian(rdd).collect())\n",
      " |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
      " |\n",
      " |  checkpoint(self) -> None\n",
      " |      Mark this RDD for checkpointing. It will be saved to a file inside the\n",
      " |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n",
      " |      all references to its parent RDDs will be removed. This function must\n",
      " |      be called before any job has been executed on this RDD. It is strongly\n",
      " |      recommended that this RDD is persisted in memory, otherwise saving it\n",
      " |      on a file will require recomputation.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.isCheckpointed`\n",
      " |      :meth:`RDD.getCheckpointFile`\n",
      " |      :meth:`RDD.localCheckpoint`\n",
      " |      :meth:`SparkContext.setCheckpointDir`\n",
      " |      :meth:`SparkContext.getCheckpointDir`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd.is_checkpointed\n",
      " |      False\n",
      " |      >>> rdd.getCheckpointFile() == None\n",
      " |      True\n",
      " |\n",
      " |      >>> rdd.checkpoint()\n",
      " |      >>> rdd.is_checkpointed\n",
      " |      True\n",
      " |      >>> rdd.getCheckpointFile() == None\n",
      " |      True\n",
      " |\n",
      " |      >>> rdd.count()\n",
      " |      5\n",
      " |      >>> rdd.is_checkpointed\n",
      " |      True\n",
      " |      >>> rdd.getCheckpointFile() == None\n",
      " |      False\n",
      " |\n",
      " |  cleanShuffleDependencies(self, blocking: bool = False) -> None\n",
      " |      Removes an RDD's shuffles and it's non-persisted ancestors.\n",
      " |\n",
      " |      When running without a shuffle service, cleaning up shuffle files enables downscaling.\n",
      " |      If you use the RDD after this call, you should checkpoint and materialize it first.\n",
      " |\n",
      " |      .. versionadded:: 3.3.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      blocking : bool, optional, default False\n",
      " |         whether to block on shuffle cleanup tasks\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is a developer API.\n",
      " |\n",
      " |  coalesce(self: 'RDD[T]', numPartitions: int, shuffle: bool = False) -> 'RDD[T]'\n",
      " |      Return a new RDD that is reduced into `numPartitions` partitions.\n",
      " |\n",
      " |      .. versionadded:: 1.0.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      shuffle : bool, optional, default False\n",
      " |          whether to add a shuffle step\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` that is reduced into `numPartitions` partitions\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.repartition`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
      " |      [[1], [2, 3], [4, 5]]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
      " |      [[1, 2, 3, 4, 5]]\n",
      " |\n",
      " |  cogroup(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[ResultIterable[V], ResultIterable[U]]]]'\n",
      " |      For each key k in `self` or `other`, return a resulting RDD that\n",
      " |      contains a tuple with the list of values for that key in `self` as\n",
      " |      well as `other`.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and cogrouped values\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.groupWith`\n",
      " |      :meth:`RDD.join`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(rdd1.cogroup(rdd2).collect()))]\n",
      " |      [('a', ([1], [2])), ('b', ([4], []))]\n",
      " |\n",
      " |  collect(self: 'RDD[T]') -> List[~T]\n",
      " |      Return a list that contains all the elements in this RDD.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          a list containing all the elements\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.toLocalIterator`\n",
      " |      :meth:`pyspark.sql.DataFrame.collect`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.range(5).collect()\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> sc.parallelize([\"x\", \"y\", \"z\"]).collect()\n",
      " |      ['x', 'y', 'z']\n",
      " |\n",
      " |  collectAsMap(self: 'RDD[Tuple[K, V]]') -> Dict[~K, ~V]\n",
      " |      Return the key-value pairs in this RDD to the master as a dictionary.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`dict`\n",
      " |          a dictionary of (key, value) pairs\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.countByValue`\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting data is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
      " |      >>> m[1]\n",
      " |      2\n",
      " |      >>> m[3]\n",
      " |      4\n",
      " |\n",
      " |  collectWithJobGroup(self: 'RDD[T]', groupId: str, description: str, interruptOnCancel: bool = False) -> 'List[T]'\n",
      " |      When collect rdd, use this method to specify job group.\n",
      " |\n",
      " |      .. versionadded:: 3.0.0\n",
      " |\n",
      " |      .. deprecated:: 3.1.0\n",
      " |          Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      groupId : str\n",
      " |          The group ID to assign.\n",
      " |      description : str\n",
      " |          The description to set for the job group.\n",
      " |      interruptOnCancel : bool, optional, default False\n",
      " |          whether to interrupt jobs on job cancellation.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          a list containing all the elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.collect`\n",
      " |      :meth:`SparkContext.setJobGroup`\n",
      " |\n",
      " |  combineByKey(self: 'RDD[Tuple[K, V]]', createCombiner: Callable[[~V], ~U], mergeValue: Callable[[~U, ~V], ~U], mergeCombiners: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f6ca241f9c0>) -> 'RDD[Tuple[K, U]]'\n",
      " |      Generic function to combine the elements for each key using a custom\n",
      " |      set of aggregation functions.\n",
      " |\n",
      " |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
      " |      type\" C.\n",
      " |\n",
      " |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n",
      " |      modify and return their first argument instead of creating a new C.\n",
      " |\n",
      " |      In addition, users can control the partitioning of the output RDD.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      createCombiner : function\n",
      " |          a function to turns a V into a C\n",
      " |      mergeValue : function\n",
      " |          a function to merge a V into a C\n",
      " |      mergeCombiners : function\n",
      " |          a function to combine two C's into a single one\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          function to compute the partition index\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and the aggregated result for each key\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduceByKey`\n",
      " |      :meth:`RDD.aggregateByKey`\n",
      " |      :meth:`RDD.foldByKey`\n",
      " |      :meth:`RDD.groupByKey`\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      V and C can be different -- for example, one might group an RDD of type\n",
      " |          (Int, Int) into an RDD of type (Int, List[Int]).\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
      " |      >>> def to_list(a):\n",
      " |      ...     return [a]\n",
      " |      ...\n",
      " |      >>> def append(a, b):\n",
      " |      ...     a.append(b)\n",
      " |      ...     return a\n",
      " |      ...\n",
      " |      >>> def extend(a, b):\n",
      " |      ...     a.extend(b)\n",
      " |      ...     return a\n",
      " |      ...\n",
      " |      >>> sorted(rdd.combineByKey(to_list, append, extend).collect())\n",
      " |      [('a', [1, 2]), ('b', [1])]\n",
      " |\n",
      " |  count(self) -> int\n",
      " |      Return the number of elements in this RDD.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          the number of elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.countApprox`\n",
      " |      :meth:`pyspark.sql.DataFrame.count`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([2, 3, 4]).count()\n",
      " |      3\n",
      " |\n",
      " |  countApprox(self, timeout: int, confidence: float = 0.95) -> int\n",
      " |      Approximate version of count() that returns a potentially incomplete\n",
      " |      result within a timeout, even if not all tasks have finished.\n",
      " |\n",
      " |      .. versionadded:: 1.2.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      timeout : int\n",
      " |          maximum time to wait for the job, in milliseconds\n",
      " |      confidence : float\n",
      " |          the desired statistical confidence in the result\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          a potentially incomplete result, with error bounds\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.count`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> rdd.countApprox(1000, 1.0)\n",
      " |      1000\n",
      " |\n",
      " |  countApproxDistinct(self: 'RDD[T]', relativeSD: float = 0.05) -> int\n",
      " |      Return approximate number of distinct elements in the RDD.\n",
      " |\n",
      " |      .. versionadded:: 1.2.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      relativeSD : float, optional\n",
      " |          Relative accuracy. Smaller values create\n",
      " |          counters that require more space.\n",
      " |          It must be greater than 0.000017.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          approximate number of distinct elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.distinct`\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      The algorithm used is based on streamlib's implementation of\n",
      " |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n",
      " |      of The Art Cardinality Estimation Algorithm\", available here\n",
      " |      <https://doi.org/10.1145/2452376.2452456>`_.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
      " |      >>> 900 < n < 1100\n",
      " |      True\n",
      " |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
      " |      >>> 16 < n < 24\n",
      " |      True\n",
      " |\n",
      " |  countByKey(self: 'RDD[Tuple[K, V]]') -> Dict[~K, int]\n",
      " |      Count the number of elements for each key, and return the result to the\n",
      " |      master as a dictionary.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          a dictionary of (key, count) pairs\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.collectAsMap`\n",
      " |      :meth:`RDD.countByValue`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.countByKey().items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |\n",
      " |  countByValue(self: 'RDD[K]') -> Dict[~K, int]\n",
      " |      Return the count of each unique value in this RDD as a dictionary of\n",
      " |      (value, count) pairs.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          a dictionary of (value, count) pairs\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.collectAsMap`\n",
      " |      :meth:`RDD.countByKey`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
      " |      [(1, 2), (2, 3)]\n",
      " |\n",
      " |  distinct(self: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n",
      " |      Return a new RDD containing the distinct elements in this RDD.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` containing the distinct elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.countApproxDistinct`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
      " |      [1, 2, 3]\n",
      " |\n",
      " |  filter(self: 'RDD[T]', f: Callable[[~T], bool]) -> 'RDD[T]'\n",
      " |      Return a new RDD containing only the elements that satisfy a predicate.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to run on each element of the RDD\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` by applying a function to each element\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.map`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
      " |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
      " |      [2, 4]\n",
      " |\n",
      " |  first(self: 'RDD[T]') -> ~T\n",
      " |      Return the first element in this RDD.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      T\n",
      " |          the first element\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.take`\n",
      " |      :meth:`pyspark.sql.DataFrame.first`\n",
      " |      :meth:`pyspark.sql.DataFrame.head`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([2, 3, 4]).first()\n",
      " |      2\n",
      " |      >>> sc.parallelize([]).first()\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: RDD is empty\n",
      " |\n",
      " |  flatMap(self: 'RDD[T]', f: Callable[[~T], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      " |      Return a new RDD by first applying a function to all elements of this\n",
      " |      RDD, and then flattening the results.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to turn a T into a sequence of U\n",
      " |      preservesPartitioning : bool, optional, default False\n",
      " |          indicates whether the input function preserves the partitioner,\n",
      " |          which should be False unless this is a pair RDD and the input\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` by applying a function to all elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.map`\n",
      " |      :meth:`RDD.mapPartitions`\n",
      " |      :meth:`RDD.mapPartitionsWithIndex`\n",
      " |      :meth:`RDD.mapPartitionsWithSplit`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([2, 3, 4])\n",
      " |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
      " |      [1, 1, 1, 2, 2, 3]\n",
      " |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
      " |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
      " |\n",
      " |  flatMapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], Iterable[~U]]) -> 'RDD[Tuple[K, U]]'\n",
      " |      Pass each value in the key-value pair RDD through a flatMap function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |         a function to turn a V into a sequence of U\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and the flat-mapped value\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.flatMap`\n",
      " |      :meth:`RDD.mapValues`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
      " |      >>> def f(x): return x\n",
      " |      ...\n",
      " |      >>> rdd.flatMapValues(f).collect()\n",
      " |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
      " |\n",
      " |  fold(self: 'RDD[T]', zeroValue: ~T, op: Callable[[~T, ~T], ~T]) -> ~T\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given associative function and a neutral \"zero value.\"\n",
      " |\n",
      " |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify ``t2``.\n",
      " |\n",
      " |      This behaves somewhat differently from fold operations implemented\n",
      " |      for non-distributed collections in functional languages like Scala.\n",
      " |      This fold operation may be applied to partitions individually, and then\n",
      " |      fold those results into the final result, rather than apply the fold\n",
      " |      to each element sequentially in some defined ordering. For functions\n",
      " |      that are not commutative, the result may differ from that of a fold\n",
      " |      applied to a non-distributed collection.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      zeroValue : T\n",
      " |          the initial value for the accumulated result of each partition\n",
      " |      op : function\n",
      " |          a function used to both accumulate results within a partition and combine\n",
      " |          results from different partitions\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      T\n",
      " |          the aggregated result\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduce`\n",
      " |      :meth:`RDD.aggregate`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
      " |      15\n",
      " |\n",
      " |  foldByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~V, func: Callable[[~V, ~V], ~V], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f6ca241f9c0>) -> 'RDD[Tuple[K, V]]'\n",
      " |      Merge the values for each key using an associative function \"func\"\n",
      " |      and a neutral \"zeroValue\" which may be added to the result an\n",
      " |      arbitrary number of times, and must not change the result\n",
      " |      (e.g., 0 for addition, or 1 for multiplication.).\n",
      " |\n",
      " |      .. versionadded:: 1.1.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      zeroValue : V\n",
      " |          the initial value for the accumulated result of each partition\n",
      " |      func : function\n",
      " |          a function to combine two V's into a single one\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          function to compute the partition index\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and the aggregated result for each key\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduceByKey`\n",
      " |      :meth:`RDD.combineByKey`\n",
      " |      :meth:`RDD.aggregateByKey`\n",
      " |      :meth:`RDD.groupByKey`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> from operator import add\n",
      " |      >>> sorted(rdd.foldByKey(0, add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |\n",
      " |  foreach(self: 'RDD[T]', f: Callable[[~T], NoneType]) -> None\n",
      " |      Applies a function to all elements of this RDD.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function applied to each element\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.foreachPartition`\n",
      " |      :meth:`pyspark.sql.DataFrame.foreach`\n",
      " |      :meth:`pyspark.sql.DataFrame.foreachPartition`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def f(x): print(x)\n",
      " |      ...\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
      " |\n",
      " |  foreachPartition(self: 'RDD[T]', f: Callable[[Iterable[~T]], NoneType]) -> None\n",
      " |      Applies a function to each partition of this RDD.\n",
      " |\n",
      " |      .. versionadded:: 1.0.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function applied to each partition\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.foreach`\n",
      " |      :meth:`pyspark.sql.DataFrame.foreach`\n",
      " |      :meth:`pyspark.sql.DataFrame.foreachPartition`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def f(iterator):\n",
      " |      ...     for x in iterator:\n",
      " |      ...          print(x)\n",
      " |      ...\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
      " |\n",
      " |  fullOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[Optional[V], Optional[U]]]]'\n",
      " |      Perform a right outer join of `self` and `other`.\n",
      " |\n",
      " |      For each element (k, v) in `self`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
      " |      (k, (v, None)) if no elements in `other` have key k.\n",
      " |\n",
      " |      Similarly, for each element (k, w) in `other`, the resulting RDD will\n",
      " |      either contain all pairs (k, (v, w)) for v in `self`, or the pair\n",
      " |      (k, (None, w)) if no elements in `self` have key k.\n",
      " |\n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |\n",
      " |      .. versionadded:: 1.2.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing all pairs of elements with matching keys\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.join`\n",
      " |      :meth:`RDD.leftOuterJoin`\n",
      " |      :meth:`RDD.fullOuterJoin`\n",
      " |      :meth:`pyspark.sql.DataFrame.join`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
      " |      >>> sorted(rdd1.fullOuterJoin(rdd2).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n",
      " |\n",
      " |  getCheckpointFile(self) -> Optional[str]\n",
      " |      Gets the name of the file to which this RDD was checkpointed\n",
      " |\n",
      " |      Not defined if RDD is checkpointed locally.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          the name of the file to which this :class:`RDD` was checkpointed\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.checkpoint`\n",
      " |      :meth:`SparkContext.setCheckpointDir`\n",
      " |      :meth:`SparkContext.getCheckpointDir`\n",
      " |\n",
      " |  getNumPartitions(self) -> int\n",
      " |      Returns the number of partitions in RDD\n",
      " |\n",
      " |      .. versionadded:: 1.1.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          number of partitions\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> rdd.getNumPartitions()\n",
      " |      2\n",
      " |\n",
      " |  getResourceProfile(self) -> Optional[pyspark.resource.profile.ResourceProfile]\n",
      " |      Get the :class:`pyspark.resource.ResourceProfile` specified with this RDD or None\n",
      " |      if it wasn't specified.\n",
      " |\n",
      " |      .. versionadded:: 3.1.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      class:`pyspark.resource.ResourceProfile`\n",
      " |          The user specified profile or None if none were specified\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.withResources`\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental\n",
      " |\n",
      " |  getStorageLevel(self) -> pyspark.storagelevel.StorageLevel\n",
      " |      Get the RDD's current storage level.\n",
      " |\n",
      " |      .. versionadded:: 1.0.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StorageLevel`\n",
      " |          current :class:`StorageLevel`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.name`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1,2])\n",
      " |      >>> rdd.getStorageLevel()\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> print(rdd.getStorageLevel())\n",
      " |      Serialized 1x Replicated\n",
      " |\n",
      " |  glom(self: 'RDD[T]') -> 'RDD[List[T]]'\n",
      " |      Return an RDD created by coalescing all elements within each partition\n",
      " |      into a list.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` coalescing all elements within each partition into a list\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1, 2], [3, 4]]\n",
      " |\n",
      " |  groupBy(self: 'RDD[T]', f: Callable[[~T], ~K], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f6ca241f9c0>) -> 'RDD[Tuple[K, Iterable[T]]]'\n",
      " |      Return an RDD of grouped items.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to compute the key\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          a function to compute the partition index\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` of grouped items\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.groupByKey`\n",
      " |      :meth:`pyspark.sql.DataFrame.groupBy`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
      " |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
      " |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n",
      " |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
      " |\n",
      " |  groupByKey(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f6ca241f9c0>) -> 'RDD[Tuple[K, Iterable[V]]]'\n",
      " |      Group the values for each key in the RDD into a single sequence.\n",
      " |      Hash-partitions the resulting RDD with numPartitions partitions.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          function to compute the partition index\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and the grouped result for each key\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduceByKey`\n",
      " |      :meth:`RDD.combineByKey`\n",
      " |      :meth:`RDD.aggregateByKey`\n",
      " |      :meth:`RDD.foldByKey`\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      If you are grouping in order to perform an aggregation (such as a\n",
      " |      sum or average) over each key, using reduceByKey or aggregateByKey will\n",
      " |      provide much better performance.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n",
      " |      [('a', [1, 1]), ('b', [1])]\n",
      " |\n",
      " |  groupWith(self: 'RDD[Tuple[Any, Any]]', other: 'RDD[Tuple[Any, Any]]', *others: 'RDD[Tuple[Any, Any]]') -> 'RDD[Tuple[Any, Tuple[ResultIterable[Any], ...]]]'\n",
      " |      Alias for cogroup but with support for multiple RDDs.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      others : :class:`RDD`\n",
      " |          other :class:`RDD`\\s\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and cogrouped values\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.cogroup`\n",
      " |      :meth:`RDD.join`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> rdd3 = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> rdd4 = sc.parallelize([(\"b\", 42)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in\n",
      " |      ...     sorted(list(rdd1.groupWith(rdd2, rdd3, rdd4).collect()))]\n",
      " |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n",
      " |\n",
      " |  histogram(self: 'RDD[S]', buckets: Union[int, List[ForwardRef('S')], Tuple[ForwardRef('S'), ...]]) -> Tuple[Sequence[ForwardRef('S')], List[int]]\n",
      " |      Compute a histogram using the provided buckets. The buckets\n",
      " |      are all open to the right except for the last which is closed.\n",
      " |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
      " |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
      " |      and 50 we would have a histogram of 1,0,1.\n",
      " |\n",
      " |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
      " |      this can be switched from an O(log n) insertion to O(1) per\n",
      " |      element (where n is the number of buckets).\n",
      " |\n",
      " |      Buckets must be sorted, not contain any duplicates, and have\n",
      " |      at least two elements.\n",
      " |\n",
      " |      If `buckets` is a number, it will generate buckets which are\n",
      " |      evenly spaced between the minimum and maximum of the RDD. For\n",
      " |      example, if the min value is 0 and the max is 100, given `buckets`\n",
      " |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n",
      " |      be at least 1. An exception is raised if the RDD contains infinity.\n",
      " |      If the elements in the RDD do not vary (max == min), a single bucket\n",
      " |      will be used.\n",
      " |\n",
      " |      .. versionadded:: 1.2.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      buckets : int, or list, or tuple\n",
      " |          if `buckets` is a number, it computes a histogram of the data using\n",
      " |          `buckets` number of buckets evenly, otherwise, `buckets` is the provided\n",
      " |          buckets to bin the data.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      tuple\n",
      " |          a tuple of buckets and histogram\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.stats`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(51))\n",
      " |      >>> rdd.histogram(2)\n",
      " |      ([0, 25, 50], [25, 26])\n",
      " |      >>> rdd.histogram([0, 5, 25, 50])\n",
      " |      ([0, 5, 25, 50], [5, 20, 26])\n",
      " |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
      " |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
      " |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
      " |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
      " |      (('a', 'b', 'c'), [2, 2])\n",
      " |\n",
      " |  id(self) -> int\n",
      " |      A unique ID for this RDD (within its SparkContext).\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          The unique ID for this :class:`RDD`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd.id()  # doctest: +SKIP\n",
      " |      3\n",
      " |\n",
      " |  intersection(self: 'RDD[T]', other: 'RDD[T]') -> 'RDD[T]'\n",
      " |      Return the intersection of this RDD and another one. The output will\n",
      " |      not contain any duplicate elements, even if the input RDDs did.\n",
      " |\n",
      " |      .. versionadded:: 1.0.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          the intersection of this :class:`RDD` and another one\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`pyspark.sql.DataFrame.intersect`\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method performs a shuffle internally.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
      " |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
      " |      >>> rdd1.intersection(rdd2).collect()\n",
      " |      [1, 2, 3]\n",
      " |\n",
      " |  isCheckpointed(self) -> bool\n",
      " |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          whether this :class:`RDD` is checkpointed and materialized, either reliably or locally\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.checkpoint`\n",
      " |      :meth:`RDD.getCheckpointFile`\n",
      " |      :meth:`SparkContext.setCheckpointDir`\n",
      " |      :meth:`SparkContext.getCheckpointDir`\n",
      " |\n",
      " |  isEmpty(self) -> bool\n",
      " |      Returns true if and only if the RDD contains no elements at all.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          whether the :class:`RDD` is empty\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.first`\n",
      " |      :meth:`pyspark.sql.DataFrame.isEmpty`\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      An RDD may be empty even when it has at least 1 partition.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([]).isEmpty()\n",
      " |      True\n",
      " |      >>> sc.parallelize([1]).isEmpty()\n",
      " |      False\n",
      " |\n",
      " |  isLocallyCheckpointed(self) -> bool\n",
      " |      Return whether this RDD is marked for local checkpointing.\n",
      " |\n",
      " |      Exposed for testing.\n",
      " |\n",
      " |      .. versionadded:: 2.2.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          whether this :class:`RDD` is marked for local checkpointing\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.localCheckpoint`\n",
      " |\n",
      " |  join(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[V, U]]]'\n",
      " |      Return an RDD containing all pairs of elements with matching keys in\n",
      " |      `self` and `other`.\n",
      " |\n",
      " |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
      " |      (k, v1) is in `self` and (k, v2) is in `other`.\n",
      " |\n",
      " |      Performs a hash join across the cluster.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing all pairs of elements with matching keys\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.leftOuterJoin`\n",
      " |      :meth:`RDD.rightOuterJoin`\n",
      " |      :meth:`RDD.fullOuterJoin`\n",
      " |      :meth:`RDD.cogroup`\n",
      " |      :meth:`RDD.groupWith`\n",
      " |      :meth:`pyspark.sql.DataFrame.join`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
      " |      >>> sorted(rdd1.join(rdd2).collect())\n",
      " |      [('a', (1, 2)), ('a', (1, 3))]\n",
      " |\n",
      " |  keyBy(self: 'RDD[T]', f: Callable[[~T], ~K]) -> 'RDD[Tuple[K, T]]'\n",
      " |      Creates tuples of the elements in this RDD by applying `f`.\n",
      " |\n",
      " |      .. versionadded:: 0.9.1\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to compute the key\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` with the elements from this that are not in `other`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.map`\n",
      " |      :meth:`RDD.keys`\n",
      " |      :meth:`RDD.values`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
      " |      >>> rdd2 = sc.parallelize(zip(range(0,5), range(0,5)))\n",
      " |      >>> [(x, list(map(list, y))) for x, y in sorted(rdd1.cogroup(rdd2).collect())]\n",
      " |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n",
      " |\n",
      " |  keys(self: 'RDD[Tuple[K, V]]') -> 'RDD[K]'\n",
      " |      Return an RDD with the keys of each tuple.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` only containing the keys\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.values`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
      " |      >>> rdd.collect()\n",
      " |      [1, 3]\n",
      " |\n",
      " |  leftOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[V, Optional[U]]]]'\n",
      " |      Perform a left outer join of `self` and `other`.\n",
      " |\n",
      " |      For each element (k, v) in `self`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
      " |      (k, (v, None)) if no elements in `other` have key k.\n",
      " |\n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing all pairs of elements with matching keys\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.join`\n",
      " |      :meth:`RDD.rightOuterJoin`\n",
      " |      :meth:`RDD.fullOuterJoin`\n",
      " |      :meth:`pyspark.sql.DataFrame.join`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(rdd1.leftOuterJoin(rdd2).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None))]\n",
      " |\n",
      " |  localCheckpoint(self) -> None\n",
      " |      Mark this RDD for local checkpointing using Spark's existing caching layer.\n",
      " |\n",
      " |      This method is for users who wish to truncate RDD lineages while skipping the expensive\n",
      " |      step of replicating the materialized data in a reliable distributed file system. This is\n",
      " |      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n",
      " |\n",
      " |      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n",
      " |      data is written to ephemeral local storage in the executors instead of to a reliable,\n",
      " |      fault-tolerant storage. The effect is that if an executor fails during the computation,\n",
      " |      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n",
      " |\n",
      " |      This is NOT safe to use with dynamic allocation, which removes executors along\n",
      " |      with their cached blocks. If you must use both features, you are advised to set\n",
      " |      `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.\n",
      " |\n",
      " |      The checkpoint directory set through :meth:`SparkContext.setCheckpointDir` is not used.\n",
      " |\n",
      " |      .. versionadded:: 2.2.0\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.checkpoint`\n",
      " |      :meth:`RDD.isLocallyCheckpointed`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd.isLocallyCheckpointed()\n",
      " |      False\n",
      " |\n",
      " |      >>> rdd.localCheckpoint()\n",
      " |      >>> rdd.isLocallyCheckpointed()\n",
      " |      True\n",
      " |\n",
      " |  lookup(self: 'RDD[Tuple[K, V]]', key: ~K) -> List[~V]\n",
      " |      Return the list of values in the RDD for key `key`. This operation\n",
      " |      is done efficiently if the RDD has a known partitioner by only\n",
      " |      searching the partition that the key maps to.\n",
      " |\n",
      " |      .. versionadded:: 1.2.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : K\n",
      " |          the key to look up\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          the list of values in the :class:`RDD` for key `key`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> l = range(1000)\n",
      " |      >>> rdd = sc.parallelize(zip(l, l), 10)\n",
      " |      >>> rdd.lookup(42)  # slow\n",
      " |      [42]\n",
      " |      >>> sorted = rdd.sortByKey()\n",
      " |      >>> sorted.lookup(42)  # fast\n",
      " |      [42]\n",
      " |      >>> sorted.lookup(1024)\n",
      " |      []\n",
      " |      >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n",
      " |      >>> list(rdd2.lookup(('a', 'b'))[0])\n",
      " |      ['c']\n",
      " |\n",
      " |  map(self: 'RDD[T]', f: Callable[[~T], ~U], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      " |      Return a new RDD by applying a function to each element of this RDD.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to run on each element of the RDD\n",
      " |      preservesPartitioning : bool, optional, default False\n",
      " |          indicates whether the input function preserves the partitioner,\n",
      " |          which should be False unless this is a pair RDD and the input\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` by applying a function to all elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.flatMap`\n",
      " |      :meth:`RDD.mapPartitions`\n",
      " |      :meth:`RDD.mapPartitionsWithIndex`\n",
      " |      :meth:`RDD.mapPartitionsWithSplit`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
      " |      [('a', 1), ('b', 1), ('c', 1)]\n",
      " |\n",
      " |  mapPartitions(self: 'RDD[T]', f: Callable[[Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      " |      Return a new RDD by applying a function to each partition of this RDD.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to run on each partition of the RDD\n",
      " |      preservesPartitioning : bool, optional, default False\n",
      " |          indicates whether the input function preserves the partitioner,\n",
      " |          which should be False unless this is a pair RDD and the input\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` by applying a function to each partition\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.map`\n",
      " |      :meth:`RDD.flatMap`\n",
      " |      :meth:`RDD.mapPartitionsWithIndex`\n",
      " |      :meth:`RDD.mapPartitionsWithSplit`\n",
      " |      :meth:`RDDBarrier.mapPartitions`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> def f(iterator): yield sum(iterator)\n",
      " |      ...\n",
      " |      >>> rdd.mapPartitions(f).collect()\n",
      " |      [3, 7]\n",
      " |\n",
      " |  mapPartitionsWithIndex(self: 'RDD[T]', f: Callable[[int, Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to run on each partition of the RDD\n",
      " |      preservesPartitioning : bool, optional, default False\n",
      " |          indicates whether the input function preserves the partitioner,\n",
      " |          which should be False unless this is a pair RDD and the input\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` by applying a function to each partition\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.map`\n",
      " |      :meth:`RDD.flatMap`\n",
      " |      :meth:`RDD.mapPartitions`\n",
      " |      :meth:`RDD.mapPartitionsWithSplit`\n",
      " |      :meth:`RDDBarrier.mapPartitionsWithIndex`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      ...\n",
      " |      >>> rdd.mapPartitionsWithIndex(f).sum()\n",
      " |      6\n",
      " |\n",
      " |  mapPartitionsWithSplit(self: 'RDD[T]', f: Callable[[int, Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      .. deprecated:: 0.9.0\n",
      " |          use meth:`RDD.mapPartitionsWithIndex` instead.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to run on each partition of the RDD\n",
      " |      preservesPartitioning : bool, optional, default False\n",
      " |          indicates whether the input function preserves the partitioner,\n",
      " |          which should be False unless this is a pair RDD and the input\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` by applying a function to each partition\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.map`\n",
      " |      :meth:`RDD.flatMap`\n",
      " |      :meth:`RDD.mapPartitions`\n",
      " |      :meth:`RDD.mapPartitionsWithIndex`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      ...\n",
      " |      >>> rdd.mapPartitionsWithSplit(f).sum()\n",
      " |      6\n",
      " |\n",
      " |  mapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], ~U]) -> 'RDD[Tuple[K, U]]'\n",
      " |      Pass each value in the key-value pair RDD through a map function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |         a function to turn a V into a U\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and the mapped value\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.map`\n",
      " |      :meth:`RDD.flatMapValues`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
      " |      >>> def f(x): return len(x)\n",
      " |      ...\n",
      " |      >>> rdd.mapValues(f).collect()\n",
      " |      [('a', 3), ('b', 1)]\n",
      " |\n",
      " |  max(self: 'RDD[T]', key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> ~T\n",
      " |      Find the maximum item in this RDD.\n",
      " |\n",
      " |      .. versionadded:: 1.0.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : function, optional\n",
      " |          A function used to generate key for comparing\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      T\n",
      " |          the maximum item\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.min`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.max()\n",
      " |      43.0\n",
      " |      >>> rdd.max(key=str)\n",
      " |      5.0\n",
      " |\n",
      " |  mean(self: 'RDD[NumberOrArray]') -> float\n",
      " |      Compute the mean of this RDD's elements.\n",
      " |\n",
      " |      .. versionadded:: 0.9.1\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          the mean of all elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.stats`\n",
      " |      :meth:`RDD.sum`\n",
      " |      :meth:`RDD.meanApprox`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).mean()\n",
      " |      2.0\n",
      " |\n",
      " |  meanApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.rdd.BoundedFloat\n",
      " |      Approximate operation to return the mean within a timeout\n",
      " |      or meet the confidence.\n",
      " |\n",
      " |      .. versionadded:: 1.2.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      timeout : int\n",
      " |          maximum time to wait for the job, in milliseconds\n",
      " |      confidence : float\n",
      " |          the desired statistical confidence in the result\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`BoundedFloat`\n",
      " |          a potentially incomplete result, with error bounds\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.mean`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000)) / 1000.0\n",
      " |      >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |\n",
      " |  min(self: 'RDD[T]', key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> ~T\n",
      " |      Find the minimum item in this RDD.\n",
      " |\n",
      " |      .. versionadded:: 1.0.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : function, optional\n",
      " |          A function used to generate key for comparing\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      T\n",
      " |          the minimum item\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.max`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.min()\n",
      " |      2.0\n",
      " |      >>> rdd.min(key=str)\n",
      " |      10.0\n",
      " |\n",
      " |  name(self) -> Optional[str]\n",
      " |      Return the name of this RDD.\n",
      " |\n",
      " |      .. versionadded:: 1.0.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          :class:`RDD` name\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.setName`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd.name() == None\n",
      " |      True\n",
      " |\n",
      " |  partitionBy(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int], partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f6ca241f9c0>) -> 'RDD[Tuple[K, V]]'\n",
      " |      Return a copy of the RDD partitioned using the specified partitioner.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          function to compute the partition index\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` partitioned using the specified partitioner\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.repartition`\n",
      " |      :meth:`RDD.repartitionAndSortWithinPartitions`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
      " |      >>> sets = pairs.partitionBy(2).glom().collect()\n",
      " |      >>> len(set(sets[0]).intersection(set(sets[1])))\n",
      " |      0\n",
      " |\n",
      " |  persist(self: 'RDD[T]', storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(False, True, False, False, 1)) -> 'RDD[T]'\n",
      " |      Set this RDD's storage level to persist its values across operations\n",
      " |      after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the RDD does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (`MEMORY_ONLY`).\n",
      " |\n",
      " |      .. versionadded:: 0.9.1\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      storageLevel : :class:`StorageLevel`, default `MEMORY_ONLY`\n",
      " |          the target storage level\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          The same :class:`RDD` with storage level set to `storageLevel`.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.cache`\n",
      " |      :meth:`RDD.unpersist`\n",
      " |      :meth:`RDD.getStorageLevel`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> rdd.persist().is_cached\n",
      " |      True\n",
      " |      >>> str(rdd.getStorageLevel())\n",
      " |      'Memory Serialized 1x Replicated'\n",
      " |      >>> _ = rdd.unpersist()\n",
      " |      >>> rdd.is_cached\n",
      " |      False\n",
      " |\n",
      " |      >>> from pyspark import StorageLevel\n",
      " |      >>> rdd2 = sc.range(5)\n",
      " |      >>> _ = rdd2.persist(StorageLevel.MEMORY_AND_DISK)\n",
      " |      >>> rdd2.is_cached\n",
      " |      True\n",
      " |      >>> str(rdd2.getStorageLevel())\n",
      " |      'Disk Memory Serialized 1x Replicated'\n",
      " |\n",
      " |      Can not override existing storage level\n",
      " |\n",
      " |      >>> _ = rdd2.persist(StorageLevel.MEMORY_ONLY_2)\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      py4j.protocol.Py4JJavaError: ...\n",
      " |\n",
      " |      Assign another storage level after `unpersist`\n",
      " |\n",
      " |      >>> _ = rdd2.unpersist()\n",
      " |      >>> rdd2.is_cached\n",
      " |      False\n",
      " |      >>> _ = rdd2.persist(StorageLevel.MEMORY_ONLY_2)\n",
      " |      >>> str(rdd2.getStorageLevel())\n",
      " |      'Memory Serialized 2x Replicated'\n",
      " |      >>> rdd2.is_cached\n",
      " |      True\n",
      " |      >>> _ = rdd2.unpersist()\n",
      " |\n",
      " |  pipe(self, command: str, env: Optional[Dict[str, str]] = None, checkCode: bool = False) -> 'RDD[str]'\n",
      " |      Return an RDD created by piping elements to a forked external process.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      command : str\n",
      " |          command to run.\n",
      " |      env : dict, optional\n",
      " |          environment variables to set.\n",
      " |      checkCode : bool, optional\n",
      " |          whether to check the return value of the shell command.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` of strings\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
      " |      ['1', '2', '', '3']\n",
      " |\n",
      " |  randomSplit(self: 'RDD[T]', weights: Sequence[Union[int, float]], seed: Optional[int] = None) -> 'List[RDD[T]]'\n",
      " |      Randomly splits this RDD with the provided weights.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      weights : list\n",
      " |          weights for splits, will be normalized if they don't sum to 1\n",
      " |      seed : int, optional\n",
      " |          random seed\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          split :class:`RDD`\\s in a list\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`pyspark.sql.DataFrame.randomSplit`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(500), 1)\n",
      " |      >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
      " |      >>> len(rdd1.collect() + rdd2.collect())\n",
      " |      500\n",
      " |      >>> 150 < rdd1.count() < 250\n",
      " |      True\n",
      " |      >>> 250 < rdd2.count() < 350\n",
      " |      True\n",
      " |\n",
      " |  reduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T]) -> ~T\n",
      " |      Reduces the elements of this RDD using the specified commutative and\n",
      " |      associative binary operator. Currently reduces partitions locally.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          the reduce function\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      T\n",
      " |          the aggregated result\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.treeReduce`\n",
      " |      :meth:`RDD.aggregate`\n",
      " |      :meth:`RDD.treeAggregate`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
      " |      15\n",
      " |      >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
      " |      10\n",
      " |      >>> sc.parallelize([]).reduce(add)\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: Can not reduce() empty RDD\n",
      " |\n",
      " |  reduceByKey(self: 'RDD[Tuple[K, V]]', func: Callable[[~V, ~V], ~V], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f6ca241f9c0>) -> 'RDD[Tuple[K, V]]'\n",
      " |      Merge the values for each key using an associative and commutative reduce function.\n",
      " |\n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |\n",
      " |      Output will be partitioned with `numPartitions` partitions, or\n",
      " |      the default parallelism level if `numPartitions` is not specified.\n",
      " |      Default partitioner is hash-partition.\n",
      " |\n",
      " |      .. versionadded:: 1.6.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          the reduce function\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          function to compute the partition index\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and the aggregated result for each key\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduceByKeyLocally`\n",
      " |      :meth:`RDD.combineByKey`\n",
      " |      :meth:`RDD.aggregateByKey`\n",
      " |      :meth:`RDD.foldByKey`\n",
      " |      :meth:`RDD.groupByKey`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKey(add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |\n",
      " |  reduceByKeyLocally(self: 'RDD[Tuple[K, V]]', func: Callable[[~V, ~V], ~V]) -> Dict[~K, ~V]\n",
      " |      Merge the values for each key using an associative and commutative reduce function, but\n",
      " |      return the results immediately to the master as a dictionary.\n",
      " |\n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          the reduce function\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          a dict containing the keys and the aggregated result for each key\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduceByKey`\n",
      " |      :meth:`RDD.aggregateByKey`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKeyLocally(add).items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |\n",
      " |  repartition(self: 'RDD[T]', numPartitions: int) -> 'RDD[T]'\n",
      " |       Return a new RDD that has exactly numPartitions partitions.\n",
      " |\n",
      " |       Can increase or decrease the level of parallelism in this RDD.\n",
      " |       Internally, this uses a shuffle to redistribute data.\n",
      " |       If you are decreasing the number of partitions in this RDD, consider\n",
      " |       using `coalesce`, which can avoid performing a shuffle.\n",
      " |\n",
      " |      .. versionadded:: 1.0.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` with exactly numPartitions partitions\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.coalesce`\n",
      " |      :meth:`RDD.partitionBy`\n",
      " |      :meth:`RDD.repartitionAndSortWithinPartitions`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |       >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
      " |       >>> sorted(rdd.glom().collect())\n",
      " |       [[1], [2, 3], [4, 5], [6, 7]]\n",
      " |       >>> len(rdd.repartition(2).glom().collect())\n",
      " |       2\n",
      " |       >>> len(rdd.repartition(10).glom().collect())\n",
      " |       10\n",
      " |\n",
      " |  repartitionAndSortWithinPartitions(self: 'RDD[Tuple[Any, Any]]', numPartitions: Optional[int] = None, partitionFunc: Callable[[Any], int] = <function portable_hash at 0x7f6ca241f9c0>, ascending: bool = True, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x7f6c6e678900>) -> 'RDD[Tuple[Any, Any]]'\n",
      " |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
      " |      sort records by their keys.\n",
      " |\n",
      " |      .. versionadded:: 1.2.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          a function to compute the partition index\n",
      " |      ascending : bool, optional, default True\n",
      " |          sort the keys in ascending or descending order\n",
      " |      keyfunc : function, optional, default identity mapping\n",
      " |          a function to compute the key\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.repartition`\n",
      " |      :meth:`RDD.partitionBy`\n",
      " |      :meth:`RDD.sortBy`\n",
      " |      :meth:`RDD.sortByKey`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
      " |      >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n",
      " |      >>> rdd2.glom().collect()\n",
      " |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
      " |\n",
      " |  rightOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[Optional[V], U]]]'\n",
      " |      Perform a right outer join of `self` and `other`.\n",
      " |\n",
      " |      For each element (k, w) in `other`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
      " |      if no elements in `self` have key k.\n",
      " |\n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing all pairs of elements with matching keys\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.join`\n",
      " |      :meth:`RDD.leftOuterJoin`\n",
      " |      :meth:`RDD.fullOuterJoin`\n",
      " |      :meth:`pyspark.sql.DataFrame.join`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(rdd2.rightOuterJoin(rdd1).collect())\n",
      " |      [('a', (2, 1)), ('b', (None, 4))]\n",
      " |\n",
      " |  sample(self: 'RDD[T]', withReplacement: bool, fraction: float, seed: Optional[int] = None) -> 'RDD[T]'\n",
      " |      Return a sampled subset of this RDD.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      withReplacement : bool\n",
      " |          can elements be sampled multiple times (replaced when sampled out)\n",
      " |      fraction : float\n",
      " |          expected size of the sample as a fraction of this RDD's size\n",
      " |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
      " |          with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
      " |      seed : int, optional\n",
      " |          seed for the random number generator\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` containing a sampled subset of elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.takeSample`\n",
      " |      :meth:`RDD.sampleByKey`\n",
      " |      :meth:`pyspark.sql.DataFrame.sample`\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |      count of the given :class:`DataFrame`.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(100), 4)\n",
      " |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n",
      " |      True\n",
      " |\n",
      " |  sampleByKey(self: 'RDD[Tuple[K, V]]', withReplacement: bool, fractions: Dict[~K, Union[float, int]], seed: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n",
      " |      Return a subset of this RDD sampled by key (via stratified sampling).\n",
      " |      Create a sample of this RDD using variable sampling rates for\n",
      " |      different keys as specified by fractions, a key to sampling rate map.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      withReplacement : bool\n",
      " |          whether to sample with or without replacement\n",
      " |      fractions : dict\n",
      " |          map of specific keys to sampling rates\n",
      " |      seed : int, optional\n",
      " |          seed for the random number generator\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the stratified sampling result\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.sample`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n",
      " |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
      " |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
      " |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n",
      " |      True\n",
      " |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n",
      " |      True\n",
      " |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n",
      " |      True\n",
      " |\n",
      " |  sampleStdev(self: 'RDD[NumberOrArray]') -> float\n",
      " |      Compute the sample standard deviation of this RDD's elements (which\n",
      " |      corrects for bias in estimating the standard deviation by dividing by\n",
      " |      N-1 instead of N).\n",
      " |\n",
      " |      .. versionadded:: 0.9.1\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          the sample standard deviation of all elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.stats`\n",
      " |      :meth:`RDD.stdev`\n",
      " |      :meth:`RDD.variance`\n",
      " |      :meth:`RDD.sampleVariance`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n",
      " |      1.0\n",
      " |\n",
      " |  sampleVariance(self: 'RDD[NumberOrArray]') -> float\n",
      " |      Compute the sample variance of this RDD's elements (which corrects\n",
      " |      for bias in estimating the variance by dividing by N-1 instead of N).\n",
      " |\n",
      " |      .. versionadded:: 0.9.1\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          the sample variance of all elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.stats`\n",
      " |      :meth:`RDD.variance`\n",
      " |      :meth:`RDD.stdev`\n",
      " |      :meth:`RDD.sampleStdev`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n",
      " |      1.0\n",
      " |\n",
      " |  saveAsHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str] = None, valueConverter: Optional[str] = None) -> None\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
      " |\n",
      " |      .. versionadded:: 1.1.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      conf : dict\n",
      " |          Hadoop job configuration\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified classname of key converter (None by default)\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified classname of value converter (None by default)\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.hadoopRDD`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      " |      :meth:`RDD.saveAsHadoopFile`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      " |      :meth:`RDD.saveAsSequenceFile`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |\n",
      " |      Set the related classes\n",
      " |\n",
      " |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n",
      " |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n",
      " |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      " |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      " |\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     path = os.path.join(d, \"old_hadoop_file\")\n",
      " |      ...\n",
      " |      ...     # Create the conf for writing\n",
      " |      ...     write_conf = {\n",
      " |      ...         \"mapred.output.format.class\": output_format_class,\n",
      " |      ...         \"mapreduce.job.output.key.class\": key_class,\n",
      " |      ...         \"mapreduce.job.output.value.class\": value_class,\n",
      " |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n",
      " |      ...     }\n",
      " |      ...\n",
      " |      ...     # Write a temporary Hadoop file\n",
      " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      " |      ...     rdd.saveAsHadoopDataset(conf=write_conf)\n",
      " |      ...\n",
      " |      ...     # Create the conf for reading\n",
      " |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n",
      " |      ...\n",
      " |      ...     # Load this Hadoop file as an RDD\n",
      " |      ...     loaded = sc.hadoopRDD(input_format_class, key_class, value_class, conf=read_conf)\n",
      " |      ...     sorted(loaded.collect())\n",
      " |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n",
      " |\n",
      " |  saveAsHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, compressionCodecClass: Optional[str] = None) -> None\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
      " |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |\n",
      " |      .. versionadded:: 1.1.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to Hadoop file\n",
      " |      outputFormatClass : str\n",
      " |          fully qualified classname of Hadoop OutputFormat\n",
      " |          (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n",
      " |      keyClass : str, optional\n",
      " |          fully qualified classname of key Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      valueClass : str, optional\n",
      " |          fully qualified classname of value Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified classname of key converter (None by default)\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified classname of value converter (None by default)\n",
      " |      conf : dict, optional\n",
      " |          (None by default)\n",
      " |      compressionCodecClass : str\n",
      " |          fully qualified classname of the compression codec class\n",
      " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.hadoopFile`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      " |      :meth:`RDD.saveAsHadoopDataset`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      " |      :meth:`RDD.saveAsSequenceFile`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |\n",
      " |      Set the related classes\n",
      " |\n",
      " |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n",
      " |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n",
      " |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      " |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      " |\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     path = os.path.join(d, \"old_hadoop_file\")\n",
      " |      ...\n",
      " |      ...     # Write a temporary Hadoop file\n",
      " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      " |      ...     rdd.saveAsHadoopFile(path, output_format_class, key_class, value_class)\n",
      " |      ...\n",
      " |      ...     # Load this Hadoop file as an RDD\n",
      " |      ...     loaded = sc.hadoopFile(path, input_format_class, key_class, value_class)\n",
      " |      ...     sorted(loaded.collect())\n",
      " |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n",
      " |\n",
      " |  saveAsNewAPIHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str] = None, valueConverter: Optional[str] = None) -> None\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
      " |\n",
      " |      .. versionadded:: 1.1.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      conf : dict\n",
      " |          Hadoop job configuration\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified classname of key converter (None by default)\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified classname of value converter (None by default)\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.newAPIHadoopRDD`\n",
      " |      :meth:`RDD.saveAsHadoopDataset`\n",
      " |      :meth:`RDD.saveAsHadoopFile`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      " |      :meth:`RDD.saveAsSequenceFile`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |\n",
      " |      Set the related classes\n",
      " |\n",
      " |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
      " |      >>> input_format_class = \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\"\n",
      " |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      " |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      " |\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     path = os.path.join(d, \"new_hadoop_file\")\n",
      " |      ...\n",
      " |      ...     # Create the conf for writing\n",
      " |      ...     write_conf = {\n",
      " |      ...         \"mapreduce.job.outputformat.class\": (output_format_class),\n",
      " |      ...         \"mapreduce.job.output.key.class\": key_class,\n",
      " |      ...         \"mapreduce.job.output.value.class\": value_class,\n",
      " |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n",
      " |      ...     }\n",
      " |      ...\n",
      " |      ...     # Write a temporary Hadoop file\n",
      " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      " |      ...     rdd.saveAsNewAPIHadoopDataset(conf=write_conf)\n",
      " |      ...\n",
      " |      ...     # Create the conf for reading\n",
      " |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n",
      " |      ...\n",
      " |      ...     # Load this Hadoop file as an RDD\n",
      " |      ...     loaded = sc.newAPIHadoopRDD(input_format_class,\n",
      " |      ...         key_class, value_class, conf=read_conf)\n",
      " |      ...     sorted(loaded.collect())\n",
      " |      [(1, ''), (1, 'a'), (3, 'x')]\n",
      " |\n",
      " |  saveAsNewAPIHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None) -> None\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
      " |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |\n",
      " |      .. versionadded:: 1.1.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to Hadoop file\n",
      " |      outputFormatClass : str\n",
      " |          fully qualified classname of Hadoop OutputFormat\n",
      " |          (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n",
      " |      keyClass : str, optional\n",
      " |          fully qualified classname of key Writable class\n",
      " |           (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      valueClass : str, optional\n",
      " |          fully qualified classname of value Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified classname of key converter (None by default)\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified classname of value converter (None by default)\n",
      " |      conf : dict, optional\n",
      " |          Hadoop job configuration (None by default)\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.newAPIHadoopFile`\n",
      " |      :meth:`RDD.saveAsHadoopDataset`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      " |      :meth:`RDD.saveAsHadoopFile`\n",
      " |      :meth:`RDD.saveAsSequenceFile`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |\n",
      " |      Set the class of output format\n",
      " |\n",
      " |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
      " |\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     path = os.path.join(d, \"hadoop_file\")\n",
      " |      ...\n",
      " |      ...     # Write a temporary Hadoop file\n",
      " |      ...     rdd = sc.parallelize([(1, {3.0: \"bb\"}), (2, {1.0: \"aa\"}), (3, {2.0: \"dd\"})])\n",
      " |      ...     rdd.saveAsNewAPIHadoopFile(path, output_format_class)\n",
      " |      ...\n",
      " |      ...     # Load this Hadoop file as an RDD\n",
      " |      ...     sorted(sc.sequenceFile(path).collect())\n",
      " |      [(1, {3.0: 'bb'}), (2, {1.0: 'aa'}), (3, {2.0: 'dd'})]\n",
      " |\n",
      " |  saveAsPickleFile(self, path: str, batchSize: int = 10) -> None\n",
      " |      Save this RDD as a SequenceFile of serialized objects. The serializer\n",
      " |      used is :class:`pyspark.serializers.CPickleSerializer`, default batch size\n",
      " |      is 10.\n",
      " |\n",
      " |      .. versionadded:: 1.1.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to pickled file\n",
      " |      batchSize : int, optional, default 10\n",
      " |          the number of Python objects represented as a single Java object.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.pickleFile`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     path = os.path.join(d, \"pickle_file\")\n",
      " |      ...\n",
      " |      ...     # Write a temporary pickled file\n",
      " |      ...     sc.parallelize(range(10)).saveAsPickleFile(path, 3)\n",
      " |      ...\n",
      " |      ...     # Load picked file as an RDD\n",
      " |      ...     sorted(sc.pickleFile(path, 3).collect())\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |\n",
      " |  saveAsSequenceFile(self: 'RDD[Tuple[K, V]]', path: str, compressionCodecClass: Optional[str] = None) -> None\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the \"org.apache.hadoop.io.Writable\" types that we convert from the\n",
      " |      RDD's key and value types. The mechanism is as follows:\n",
      " |\n",
      " |          1. Pickle is used to convert pickled Python RDD into RDD of Java objects.\n",
      " |          2. Keys and values of this Java RDD are converted to Writables and written out.\n",
      " |\n",
      " |      .. versionadded:: 1.1.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to sequence file\n",
      " |      compressionCodecClass : str, optional\n",
      " |          fully qualified classname of the compression codec class\n",
      " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.sequenceFile`\n",
      " |      :meth:`RDD.saveAsHadoopFile`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      " |      :meth:`RDD.saveAsHadoopDataset`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      " |      :meth:`RDD.saveAsSequenceFile`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |\n",
      " |      Set the related classes\n",
      " |\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     path = os.path.join(d, \"sequence_file\")\n",
      " |      ...\n",
      " |      ...     # Write a temporary sequence file\n",
      " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      " |      ...     rdd.saveAsSequenceFile(path)\n",
      " |      ...\n",
      " |      ...     # Load this sequence file as an RDD\n",
      " |      ...     loaded = sc.sequenceFile(path)\n",
      " |      ...     sorted(loaded.collect())\n",
      " |      [(1, ''), (1, 'a'), (3, 'x')]\n",
      " |\n",
      " |  saveAsTextFile(self, path: str, compressionCodecClass: Optional[str] = None) -> None\n",
      " |      Save this RDD as a text file, using string representations of elements.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to text file\n",
      " |      compressionCodecClass : str, optional\n",
      " |          fully qualified classname of the compression codec class\n",
      " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.textFile`\n",
      " |      :meth:`SparkContext.wholeTextFiles`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      >>> from fileinput import input\n",
      " |      >>> from glob import glob\n",
      " |      >>> with tempfile.TemporaryDirectory() as d1:\n",
      " |      ...     path1 = os.path.join(d1, \"text_file1\")\n",
      " |      ...\n",
      " |      ...     # Write a temporary text file\n",
      " |      ...     sc.parallelize(range(10)).saveAsTextFile(path1)\n",
      " |      ...\n",
      " |      ...     # Load text file as an RDD\n",
      " |      ...     ''.join(sorted(input(glob(path1 + \"/part-0000*\"))))\n",
      " |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
      " |\n",
      " |      Empty lines are tolerated when saving to text files.\n",
      " |\n",
      " |      >>> with tempfile.TemporaryDirectory() as d2:\n",
      " |      ...     path2 = os.path.join(d2, \"text2_file2\")\n",
      " |      ...\n",
      " |      ...     # Write another temporary text file\n",
      " |      ...     sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(path2)\n",
      " |      ...\n",
      " |      ...     # Load text file as an RDD\n",
      " |      ...     ''.join(sorted(input(glob(path2 + \"/part-0000*\"))))\n",
      " |      '\\n\\n\\nbar\\nfoo\\n'\n",
      " |\n",
      " |      Using compressionCodecClass\n",
      " |\n",
      " |      >>> from fileinput import input, hook_compressed\n",
      " |      >>> with tempfile.TemporaryDirectory() as d3:\n",
      " |      ...     path3 = os.path.join(d3, \"text3\")\n",
      " |      ...     codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      ...\n",
      " |      ...     # Write another temporary text file with specified codec\n",
      " |      ...     sc.parallelize(['foo', 'bar']).saveAsTextFile(path3, codec)\n",
      " |      ...\n",
      " |      ...     # Load text file as an RDD\n",
      " |      ...     result = sorted(input(glob(path3 + \"/part*.gz\"), openhook=hook_compressed))\n",
      " |      ...     ''.join([r.decode('utf-8') if isinstance(r, bytes) else r for r in result])\n",
      " |      'bar\\nfoo\\n'\n",
      " |\n",
      " |  setName(self: 'RDD[T]', name: str) -> 'RDD[T]'\n",
      " |      Assign a name to this RDD.\n",
      " |\n",
      " |      .. versionadded:: 1.0.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          new name\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          the same :class:`RDD` with name updated\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.name`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2])\n",
      " |      >>> rdd.setName('I am an RDD').name()\n",
      " |      'I am an RDD'\n",
      " |\n",
      " |  sortBy(self: 'RDD[T]', keyfunc: Callable[[~T], ForwardRef('S')], ascending: bool = True, numPartitions: Optional[int] = None) -> 'RDD[T]'\n",
      " |      Sorts this RDD by the given keyfunc\n",
      " |\n",
      " |      .. versionadded:: 1.1.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keyfunc : function\n",
      " |          a function to compute the key\n",
      " |      ascending : bool, optional, default True\n",
      " |          sort the keys in ascending or descending order\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.sortByKey`\n",
      " |      :meth:`pyspark.sql.DataFrame.sort`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
      " |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |\n",
      " |  sortByKey(self: 'RDD[Tuple[K, V]]', ascending: Optional[bool] = True, numPartitions: Optional[int] = None, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x7f6c6e678cc0>) -> 'RDD[Tuple[K, V]]'\n",
      " |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
      " |\n",
      " |      .. versionadded:: 0.9.1\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ascending : bool, optional, default True\n",
      " |          sort the keys in ascending or descending order\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      keyfunc : function, optional, default identity mapping\n",
      " |          a function to compute the key\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.sortBy`\n",
      " |      :meth:`pyspark.sql.DataFrame.sort`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey().first()\n",
      " |      ('1', 3)\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
      " |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
      " |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
      " |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
      " |\n",
      " |  stats(self: 'RDD[NumberOrArray]') -> pyspark.statcounter.StatCounter\n",
      " |      Return a :class:`StatCounter` object that captures the mean, variance\n",
      " |      and count of the RDD's elements in one operation.\n",
      " |\n",
      " |      .. versionadded:: 0.9.1\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StatCounter`\n",
      " |          a :class:`StatCounter` capturing the mean, variance and count of all elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.stdev`\n",
      " |      :meth:`RDD.sampleStdev`\n",
      " |      :meth:`RDD.variance`\n",
      " |      :meth:`RDD.sampleVariance`\n",
      " |      :meth:`RDD.histogram`\n",
      " |      :meth:`pyspark.sql.DataFrame.stat`\n",
      " |\n",
      " |  stdev(self: 'RDD[NumberOrArray]') -> float\n",
      " |      Compute the standard deviation of this RDD's elements.\n",
      " |\n",
      " |      .. versionadded:: 0.9.1\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          the standard deviation of all elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.stats`\n",
      " |      :meth:`RDD.sampleStdev`\n",
      " |      :meth:`RDD.variance`\n",
      " |      :meth:`RDD.sampleVariance`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).stdev()\n",
      " |      0.816...\n",
      " |\n",
      " |  subtract(self: 'RDD[T]', other: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n",
      " |      Return each value in `self` that is not contained in `other`.\n",
      " |\n",
      " |      .. versionadded:: 0.9.1\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` with the elements from this that are not in `other`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.subtractByKey`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(rdd1.subtract(rdd2).collect())\n",
      " |      [('a', 1), ('b', 4), ('b', 5)]\n",
      " |\n",
      " |  subtractByKey(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, Any]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n",
      " |      Return each (key, value) pair in `self` that has no pair with matching\n",
      " |      key in `other`.\n",
      " |\n",
      " |      .. versionadded:: 0.9.1\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` with the pairs from this whose keys are not in `other`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.subtract`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(rdd1.subtractByKey(rdd2).collect())\n",
      " |      [('b', 4), ('b', 5)]\n",
      " |\n",
      " |  sum(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
      " |      Add up the elements in this RDD.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      float, int, or complex\n",
      " |          the sum of all elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.mean`\n",
      " |      :meth:`RDD.sumApprox`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
      " |      6.0\n",
      " |\n",
      " |  sumApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.rdd.BoundedFloat\n",
      " |      Approximate operation to return the sum within a timeout\n",
      " |      or meet the confidence.\n",
      " |\n",
      " |      .. versionadded:: 1.2.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      timeout : int\n",
      " |          maximum time to wait for the job, in milliseconds\n",
      " |      confidence : float\n",
      " |          the desired statistical confidence in the result\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`BoundedFloat`\n",
      " |          a potentially incomplete result, with error bounds\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.sum`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000))\n",
      " |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |\n",
      " |  take(self: 'RDD[T]', num: int) -> List[~T]\n",
      " |      Take the first num elements of the RDD.\n",
      " |\n",
      " |      It works by first scanning one partition, and use the results from\n",
      " |      that partition to estimate the number of additional partitions needed\n",
      " |      to satisfy the limit.\n",
      " |\n",
      " |      Translated from the Scala implementation in RDD#take().\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num : int\n",
      " |          first number of elements\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          the first `num` elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.first`\n",
      " |      :meth:`pyspark.sql.DataFrame.take`\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
      " |      [2, 3]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
      " |      [2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
      " |      [91, 92, 93]\n",
      " |\n",
      " |  takeOrdered(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n",
      " |      Get the N elements from an RDD ordered in ascending order or as\n",
      " |      specified by the optional key function.\n",
      " |\n",
      " |      .. versionadded:: 1.0.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num : int\n",
      " |          top N\n",
      " |      key : function, optional\n",
      " |          a function used to generate key for comparing\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          the top N elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.top`\n",
      " |      :meth:`RDD.max`\n",
      " |      :meth:`RDD.min`\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
      " |      [1, 2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
      " |      [10, 9, 7, 6, 5, 4]\n",
      " |      >>> sc.emptyRDD().takeOrdered(3)\n",
      " |      []\n",
      " |\n",
      " |  takeSample(self: 'RDD[T]', withReplacement: bool, num: int, seed: Optional[int] = None) -> List[~T]\n",
      " |      Return a fixed-size sampled subset of this RDD.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      withReplacement : bool\n",
      " |          whether sampling is done with replacement\n",
      " |      num : int\n",
      " |          size of the returned sample\n",
      " |      seed : int, optional\n",
      " |          random seed\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          a fixed-size sampled subset of this :class:`RDD` in an array\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.sample`\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import sys\n",
      " |      >>> rdd = sc.parallelize(range(0, 10))\n",
      " |      >>> len(rdd.takeSample(True, 20, 1))\n",
      " |      20\n",
      " |      >>> len(rdd.takeSample(False, 5, 2))\n",
      " |      5\n",
      " |      >>> len(rdd.takeSample(False, 15, 3))\n",
      " |      10\n",
      " |      >>> sc.range(0, 10).takeSample(False, sys.maxsize)\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: Sample size cannot be greater than ...\n",
      " |\n",
      " |  toDF(self, schema=None, sampleRatio=None) from pyspark.sql.session._monkey_patch_RDD.<locals>\n",
      " |      Converts current :class:`RDD` into a :class:`DataFrame`\n",
      " |\n",
      " |      This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      " |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      " |          column names, default is None.  The data type string format equals to\n",
      " |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      " |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      " |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
      " |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
      " |      sampleRatio : float, optional\n",
      " |          the sample ratio of rows used for inferring\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = spark.range(1).rdd.map(lambda x: tuple(x))\n",
      " |      >>> rdd.collect()\n",
      " |      [(0,)]\n",
      " |      >>> rdd.toDF().show()\n",
      " |      +---+\n",
      " |      | _1|\n",
      " |      +---+\n",
      " |      |  0|\n",
      " |      +---+\n",
      " |\n",
      " |  toDebugString(self) -> Optional[bytes]\n",
      " |      A description of this RDD and its recursive dependencies for debugging.\n",
      " |\n",
      " |      .. versionadded:: 1.0.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      bytes\n",
      " |          debugging information of this :class:`RDD`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd.toDebugString()\n",
      " |      b'...PythonRDD...ParallelCollectionRDD...'\n",
      " |\n",
      " |  toLocalIterator(self: 'RDD[T]', prefetchPartitions: bool = False) -> Iterator[~T]\n",
      " |      Return an iterator that contains all of the elements in this RDD.\n",
      " |      The iterator will consume as much memory as the largest partition in this RDD.\n",
      " |      With prefetch it may consume up to the memory of the 2 largest partitions.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prefetchPartitions : bool, optional\n",
      " |          If Spark should pre-fetch the next partition\n",
      " |          before it is needed.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`collections.abc.Iterator`\n",
      " |          an iterator that contains all of the elements in this :class:`RDD`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.collect`\n",
      " |      :meth:`pyspark.sql.DataFrame.toLocalIterator`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(10))\n",
      " |      >>> [x for x in rdd.toLocalIterator()]\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |\n",
      " |  top(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n",
      " |      Get the top N elements from an RDD.\n",
      " |\n",
      " |      .. versionadded:: 1.0.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num : int\n",
      " |          top N\n",
      " |      key : function, optional\n",
      " |          a function used to generate key for comparing\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          the top N elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.takeOrdered`\n",
      " |      :meth:`RDD.max`\n",
      " |      :meth:`RDD.min`\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |\n",
      " |      It returns the list sorted in descending order.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
      " |      [12]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
      " |      [6, 5]\n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
      " |      [4, 3, 2]\n",
      " |\n",
      " |  treeAggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U], depth: int = 2) -> ~U\n",
      " |      Aggregates the elements of this RDD in a multi-level tree\n",
      " |      pattern.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      zeroValue : U\n",
      " |          the initial value for the accumulated result of each partition\n",
      " |      seqOp : function\n",
      " |          a function used to accumulate results within a partition\n",
      " |      combOp : function\n",
      " |          an associative function used to combine results from different partitions\n",
      " |      depth : int, optional, default 2\n",
      " |          suggested depth of the tree\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      U\n",
      " |          the aggregated result\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.aggregate`\n",
      " |      :meth:`RDD.treeReduce`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeAggregate(0, add, add)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 10)\n",
      " |      -5\n",
      " |\n",
      " |  treeReduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T], depth: int = 2) -> ~T\n",
      " |      Reduces the elements of this RDD in a multi-level tree pattern.\n",
      " |\n",
      " |      .. versionadded:: 1.3.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          the reduce function\n",
      " |      depth : int, optional, default 2\n",
      " |          suggested depth of the tree (default: 2)\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      T\n",
      " |          the aggregated result\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduce`\n",
      " |      :meth:`RDD.aggregate`\n",
      " |      :meth:`RDD.treeAggregate`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeReduce(add)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 10)\n",
      " |      -5\n",
      " |\n",
      " |  union(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n",
      " |      Return the union of this RDD and another one.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          the union of this :class:`RDD` and another one\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.union`\n",
      " |      :meth:`pyspark.sql.DataFrame.union`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> rdd.union(rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |\n",
      " |  unpersist(self: 'RDD[T]', blocking: bool = False) -> 'RDD[T]'\n",
      " |      Mark the RDD as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |\n",
      " |      .. versionadded:: 0.9.1\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      blocking : bool, optional, default False\n",
      " |          whether to block until all blocks are deleted\n",
      " |\n",
      " |          .. versionadded:: 3.0.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          The same :class:`RDD`\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.cache`\n",
      " |      :meth:`RDD.persist`\n",
      " |      :meth:`RDD.getStorageLevel`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd.is_cached\n",
      " |      False\n",
      " |      >>> _ = rdd.unpersist()\n",
      " |      >>> rdd.is_cached\n",
      " |      False\n",
      " |      >>> _ = rdd.cache()\n",
      " |      >>> rdd.is_cached\n",
      " |      True\n",
      " |      >>> _ = rdd.unpersist()\n",
      " |      >>> rdd.is_cached\n",
      " |      False\n",
      " |      >>> _ = rdd.unpersist()\n",
      " |\n",
      " |  values(self: 'RDD[Tuple[K, V]]') -> 'RDD[V]'\n",
      " |      Return an RDD with the values of each tuple.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` only containing the values\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.keys`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(1, 2), (3, 4)]).values()\n",
      " |      >>> rdd.collect()\n",
      " |      [2, 4]\n",
      " |\n",
      " |  variance(self: 'RDD[NumberOrArray]') -> float\n",
      " |      Compute the variance of this RDD's elements.\n",
      " |\n",
      " |      .. versionadded:: 0.9.1\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          the variance of all elements\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.stats`\n",
      " |      :meth:`RDD.sampleVariance`\n",
      " |      :meth:`RDD.stdev`\n",
      " |      :meth:`RDD.sampleStdev`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).variance()\n",
      " |      0.666...\n",
      " |\n",
      " |  withResources(self: 'RDD[T]', profile: pyspark.resource.profile.ResourceProfile) -> 'RDD[T]'\n",
      " |      Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n",
      " |      This is only supported on certain cluster managers and currently requires dynamic\n",
      " |      allocation to be enabled. It will result in new executors with the resources specified\n",
      " |      being acquired to calculate the RDD.\n",
      " |\n",
      " |      .. versionadded:: 3.1.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      profile : :class:`pyspark.resource.ResourceProfile`\n",
      " |          a resource profile\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          the same :class:`RDD` with user specified profile\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.getResourceProfile`\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental\n",
      " |\n",
      " |  zip(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n",
      " |      Zips this RDD with another one, returning key-value pairs with the\n",
      " |      first element in each RDD second element in each RDD, etc. Assumes\n",
      " |      that the two RDDs have the same number of partitions and the same\n",
      " |      number of elements in each partition (e.g. one was made through\n",
      " |      a map on the other).\n",
      " |\n",
      " |      .. versionadded:: 1.0.0\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the zipped key-value pairs\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.zipWithIndex`\n",
      " |      :meth:`RDD.zipWithUniqueId`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize(range(0,5))\n",
      " |      >>> rdd2 = sc.parallelize(range(1000, 1005))\n",
      " |      >>> rdd1.zip(rdd2).collect()\n",
      " |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
      " |\n",
      " |  zipWithIndex(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n",
      " |      Zips this RDD with its element indices.\n",
      " |\n",
      " |      The ordering is first based on the partition index and then the\n",
      " |      ordering of items within each partition. So the first item in\n",
      " |      the first partition gets index 0, and the last item in the last\n",
      " |      partition receives the largest index.\n",
      " |\n",
      " |      This method needs to trigger a spark job when this RDD contains\n",
      " |      more than one partitions.\n",
      " |\n",
      " |      .. versionadded:: 1.2.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the zipped key-index pairs\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.zip`\n",
      " |      :meth:`RDD.zipWithUniqueId`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n",
      " |\n",
      " |  zipWithUniqueId(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n",
      " |      Zips this RDD with generated unique Long ids.\n",
      " |\n",
      " |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
      " |      n is the number of partitions. So there may exist gaps, but this\n",
      " |      method won't trigger a spark job, which is different from\n",
      " |      :meth:`zipWithIndex`.\n",
      " |\n",
      " |      .. versionadded:: 1.2.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the zipped key-UniqueId pairs\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.zip`\n",
      " |      :meth:`RDD.zipWithIndex`\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  context\n",
      " |      The :class:`SparkContext` that this RDD was created on.\n",
      " |\n",
      " |      .. versionadded:: 0.7.0\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`SparkContext`\n",
      " |          The :class:`SparkContext` that this RDD was created on\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd.context\n",
      " |      <SparkContext ...>\n",
      " |      >>> rdd.context is sc\n",
      " |      True\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {}\n",
      " |\n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |\n",
      " |  __parameters__ = (+T_co,)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |\n",
      " |  __class_getitem__(...)\n",
      " |      Parameterizes a generic class.\n",
      " |\n",
      " |      At least, parameterizing a generic class is the *main* thing this\n",
      " |      method does. For example, for some generic class `Foo`, this is called\n",
      " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |\n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo[T]: ...`.\n",
      " |\n",
      " |  __init_subclass__(...)\n",
      " |      Function to initialize subclasses.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(rdd0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
